{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a2gw-jJrtAU",
        "colab_type": "code",
        "outputId": "21c4a96e-8e1e-44b6-d6df-d620a25cb641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import math\n",
        "import re\n",
        "import numpy as np\n",
        "frm nltk.util import ngrams\n",
        "\n",
        "import gensim\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Activation, SimpleRNN, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.utils.data_utils import get_file"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJV_A5ZMs3Af",
        "colab_type": "code",
        "outputId": "632061c4-8826-49d2-93e5-a039eba005ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-06 12:56:10--  https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 924745 (903K) [text/plain]\n",
            "Saving to: ‘speeches.txt.1’\n",
            "\n",
            "\rspeeches.txt.1        0%[                    ]       0  --.-KB/s               \rspeeches.txt.1      100%[===================>] 903.07K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-10-06 12:56:10 (20.6 MB/s) - ‘speeches.txt.1’ saved [924745/924745]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR869ktk38cG",
        "colab_type": "text"
      },
      "source": [
        "**Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbqZ_IcSvUSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('speeches.txt') as f:\n",
        "  text = f.read()\n",
        "lower_text=nltk.sent_tokenize(text.lower())\n",
        "sentences=[]\n",
        "for line in lower_text:\n",
        "  if re.search(r'[a-z0-9]', line):\n",
        "    l=re.sub(r'[^a-z0-9]',\" \", line)\n",
        "  sentences.append(l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2F2AJx234no",
        "colab_type": "text"
      },
      "source": [
        "**Train Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evsALZ5b4P7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = sentences[:math.ceil(0.8*len(sentences))]\n",
        "test = sentences[math.ceil(0.8*len(sentences)):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDS88aLf3sFB",
        "colab_type": "text"
      },
      "source": [
        "**Classic Modelling N grams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHL-AGsW8nbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Ngram_model(n, data):\n",
        "  ngram_list=[]\n",
        "  for sentence in data:\n",
        "    sentence=nltk.word_tokenize(sentence)\n",
        "    padded_sent = list(['<s>']+sentence+['</s>'])\n",
        "    ngram_list.extend(list(ngrams(padded_sent, n=n)))\n",
        "  return ngram_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaETa0beHUTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Freq_Dist(ngram_list):\n",
        "  freq=nltk.FreqDist(ngram_list)\n",
        "  freq_dist={}\n",
        "  for key in freq:\n",
        "    freq_dist[key]=freq[key]\n",
        "  return freq_dist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Nt8_GBUPjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MLE_dict(n, data):\n",
        "  mle_dict={}\n",
        "  l1=Ngram_model(n, data)\n",
        "  f1=Freq_Dist(l1)\n",
        "  if(n!=1):\n",
        "    l2=Ngram_model(n-1, data)\n",
        "    f2=Freq_Dist(l2)\n",
        "    for key in f1:\n",
        "      x=(' '.join(key))\n",
        "      y=f1[key]/f2[key[:-1]]\n",
        "      mle_dict[x]=y\n",
        "  else:\n",
        "    for key in f1:\n",
        "      x=(' '.join(key))\n",
        "      y=f1[key]/len(l1)\n",
        "      mle_dict[x]=y\n",
        "  return mle_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8HpamywqiAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Generator(mle_dict):\n",
        "  sentence=[]\n",
        "  pvalues=np.array([mle_dict[key] for key in mle_dict])\n",
        "  pvalues=pvalues/pvalues.sum()\n",
        "  while(True):\n",
        "    poss=list(np.random.multinomial(20, pvalues))\n",
        "    start=list(mle_dict.keys())[poss.index(max(poss))]\n",
        "    if '<s>' in start:\n",
        "      break\n",
        "    else:\n",
        "      pass\n",
        "  sentence=sentence+start.split(' ')\n",
        "  while(True):\n",
        "    poss=list(np.random.multinomial(20, pvalues))\n",
        "    move=list(mle_dict.keys())[poss.index(max(poss))]\n",
        "    if('</s>' not in move and '<s>' not in move):\n",
        "      sentence=sentence+move.split(' ')\n",
        "      pass\n",
        "    elif('</s>' in move):\n",
        "      sentence=sentence+move.split(' ')\n",
        "      break\n",
        "    \n",
        "  return ' '.join(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMi2Bf8VqiRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Perplexity(n, mle_dict, test):\n",
        "  ngram_test=Ngram_model(n, test)\n",
        "  perplexity=0\n",
        "  N=len(ngram_test)\n",
        "  for key in ngram_test:\n",
        "    if key in mle_dict:\n",
        "      perplexity=perplexity-(math.log(mle_dict(key))/N)\n",
        "    else:\n",
        "      perplexity=perplexity-(math.log(1/len(mle_dict))/N)\n",
        "  perplexity=math.exp(perplexity)\n",
        "  return perplexity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7AhkUmA3V4c",
        "colab_type": "text"
      },
      "source": [
        "**Perplexity of N gram Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEVfV8p80A7q",
        "colab_type": "code",
        "outputId": "abc40aa9-ac06-4ac9-cc34-44b1bce0fafa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "for n in range(1, 5):\n",
        "  mle=MLE_dict(n, train)\n",
        "  p=Perplexity(n, mle, test)\n",
        "  print(\"Perplexity of \"+str(n)+\"-gram : \"+str(p))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity of 1-gram : 5214.999999982188\n",
            "Perplexity of 2-gram : 41145.99999998437\n",
            "Perplexity of 3-gram : 80793.00000049338\n",
            "Perplexity of 4-gram : 98677.9999992146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDH7oAQc7umV",
        "colab_type": "text"
      },
      "source": [
        "**Observation : Perplexity of unigram < bigram < trigram < quadgram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy2R_vD93ky7",
        "colab_type": "text"
      },
      "source": [
        "**Random Text Generation of N gram Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14LQAJZRa52k",
        "colab_type": "code",
        "outputId": "76c6e636-ffd4-424d-ca1e-a1dbb26367a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        }
      },
      "source": [
        "for n in range(1, 5):\n",
        "  mle=MLE_dict(n, train)\n",
        "  print(\"Generation text : \"+str(n)+\"-gram\")\n",
        "  print('')\n",
        "  for i in range(5):\n",
        "    print(Generator(mle))\n",
        "  print('')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generation text : 1-gram\n",
            "\n",
            "<s> </s>\n",
            "<s> </s>\n",
            "<s> s to </s>\n",
            "<s> to and the </s>\n",
            "<s> </s>\n",
            "\n",
            "Generation text : 2-gram\n",
            "\n",
            "<s> it executive orders place </s>\n",
            "<s> so terraza floor resources are overturn the right now feet </s>\n",
            "<s> they earlier </s>\n",
            "<s> what infrastructure </s>\n",
            "<s> you disaster </s>\n",
            "\n",
            "Generation text : 3-gram\n",
            "\n",
            "<s> containing the place is falling an army tank trafficking and the not fair because 2 trillion and to nothing </s>\n",
            "<s> it was enemies something that made him look know somebody is off stage everybody tremendous and we have jeb bush china necessary to you and when the battleships we election is tougher for the wall coming for a very short period i like the highways and somebody sending people that importing extremism through be fantastic and persuasive but we to appreciate what first we need a house </s>\n",
            "<s> in fact to be a rapidly expanded their cost of the say what </s>\n",
            "<s> cause abraham and enemies must and medicaid without than making america statistic that s mean most people dollar is getting chance cause actually discuss a rebalancing watch these teleprompters had small hands more prosperous period been allowed to are numbered </s>\n",
            "<s> your deductibles lose more and test the mettle doctor he lied mountain china is a white terraza he a great do countries </s>\n",
            "\n",
            "Generation text : 4-gram\n",
            "\n",
            "<s> thank you very you think about it to leave the table something in common </s>\n",
            "<s> phil said donald to spread universal values that more and more also a philosophical struggle at the border </s>\n",
            "<s> no purpose </s> policy disaster after another some of you have for the folks here lincoln coming home back we will win if bah bing bing bing many many times </s>\n",
            "<s> iran too would to help but they helplessly as north korea if it gets too wealth in minerals different of these actions have one game changer that ah ah i can these actions have helped and we while being fast and not just much larger number </s>\n",
            "<s> almost all of universal values that not friend of mine who vets we have to so easy to fix a great honor </s>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgEbwznc8A4C",
        "colab_type": "text"
      },
      "source": [
        "**Observation : Readability of quadgram > trigram > bigram > unigram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwhqqEw24LPN",
        "colab_type": "text"
      },
      "source": [
        "**Neural Language Modelling : LSTM and RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqF0QHcbVFUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Neural_training(data, model_name):\n",
        "  \n",
        "  max_length=8\n",
        "  sent_data=[['<s>']+line.split()[:max_length]+['</s>'] for line in data]\n",
        "  \n",
        "  word_model = gensim.models.Word2Vec(sent_data, size=10, min_count=1, iter=100)\n",
        "  trained_vocabs = word_model.wv.vocab\n",
        "  trained_weights = np.array(word_model.wv.vectors)\n",
        "  vocab_size = word_model.wv.vectors.shape[0]\n",
        "  embedding_size = word_model.wv.vectors.shape[1]\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[trained_weights]))\n",
        "  if(model_name=='LSTM'):\n",
        "    model.add(LSTM(units=embedding_size))\n",
        "  if(model_name=='RNN'):\n",
        "    model.add(SimpleRNN(units=embedding_size))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(units=vocab_size))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "  \n",
        "  corpus=[]\n",
        "  for i, line in enumerate(sent_data):\n",
        "    for j in range(len(line)-2):\n",
        "      for k in range(j+2,len(line)):\n",
        "        corpus.append(line[j:k])\n",
        "  \n",
        "  train_X = np.zeros([len(corpus), max_length+2], dtype=np.int32)\n",
        "  train_y = np.zeros([len(corpus)], dtype=np.int32)\n",
        "  for i, line in enumerate(corpus):\n",
        "    for j, word in enumerate(line[:-1]):\n",
        "      train_X[i, j] = word_model.wv.vocab[word].index\n",
        "    train_y[i]=word_model.wv.vocab[line[-1]].index\n",
        "  model.fit(train_X, train_y, batch_size=128, epochs=5)\n",
        "  \n",
        "  return (word_model, model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp5ytGuyYKt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Neural_generator(word_model, model):\n",
        "  x=['<s>']\n",
        "  i=0\n",
        "  while(i<=10):\n",
        "    i=i+1\n",
        "    windex_x=[word_model.wv.vocab[word].index for word in x]\n",
        "    preds = model.predict(x=np.array([windex_x]))\n",
        "    preds=preds/preds.sum()\n",
        "    p = np.random.multinomial(1, preds[0], 1)\n",
        "    idx = np.argmax(p)\n",
        "    w=word_model.wv.index2word[idx]\n",
        "    if(w=='</s>'):\n",
        "      x.append(w)\n",
        "      break\n",
        "    else:\n",
        "      x.append(w)\n",
        "      pass\n",
        "  return(' '.join(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLPJdNUXloe4",
        "colab_type": "text"
      },
      "source": [
        "**Perplexity of Neural Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcnamchlltCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Neural_perplexity(model, test):\n",
        "  \n",
        "  max_length=8\n",
        "  sent_data=[['<s>']+line.split()[:max_length]+['</s>'] for line in test]\n",
        "  \n",
        "  word_model = gensim.models.Word2Vec(sent_data, size=10, min_count=1, iter=100)\n",
        "  trained_vocabs = word_model.wv.vocab\n",
        "  trained_weights = np.array(word_model.wv.vectors)\n",
        "  vocab_size = word_model.wv.vectors.shape[0]\n",
        "  embedding_size = word_model.wv.vectors.shape[1]\n",
        "  \n",
        "  corpus=[]\n",
        "  for i, line in enumerate(sent_data):\n",
        "    for j in range(len(line)-2):\n",
        "      for k in range(j+2,len(line)):\n",
        "        corpus.append(line[j:k])\n",
        "        \n",
        "  N=len(corpus)\n",
        "  perplexity=0\n",
        "  for i, line in enumerate(corpus):\n",
        "    x=line[:-1]\n",
        "    y=line[-1]\n",
        "    windex_x=[word_model.wv.vocab[word].index for word in x]\n",
        "    preds=model.predict(x=np.array([windex_x]))\n",
        "    idx=word_model.wv.vocab[y].index\n",
        "    prob=preds[0][idx]\n",
        "    perplexity=perplexity-(math.log(prob)/N)\n",
        "  perplexity=math.exp(perplexity)\n",
        "  \n",
        "  return perplexity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5HMBM_o4Tdq",
        "colab_type": "text"
      },
      "source": [
        "**Random Text Generation of Neural Models using start words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O--8qZ9JXFVX",
        "colab_type": "code",
        "outputId": "94171cc4-db45-4125-d74b-3475486bfa9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "rnn_word_model, rnn_model =Neural_training(train, 'RNN')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/5\n",
            "343536/343536 [==============================] - 60s 176us/step - loss: 6.1706\n",
            "Epoch 2/5\n",
            "343536/343536 [==============================] - 58s 168us/step - loss: 5.9893\n",
            "Epoch 3/5\n",
            "343536/343536 [==============================] - 57s 167us/step - loss: 5.8743\n",
            "Epoch 4/5\n",
            "343536/343536 [==============================] - 58s 168us/step - loss: 5.8289\n",
            "Epoch 5/5\n",
            "343536/343536 [==============================] - 58s 168us/step - loss: 5.7812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnvKN6We_oNL",
        "colab_type": "code",
        "outputId": "fab477db-28f6-46a1-ddbc-04bccaca404a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "lstm_word_model, lstm_model =Neural_training(train, 'LSTM')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "343536/343536 [==============================] - 74s 216us/step - loss: 6.1104\n",
            "Epoch 2/5\n",
            "343536/343536 [==============================] - 73s 212us/step - loss: 5.6761\n",
            "Epoch 3/5\n",
            "343536/343536 [==============================] - 74s 214us/step - loss: 5.4020\n",
            "Epoch 4/5\n",
            "343536/343536 [==============================] - 74s 215us/step - loss: 5.2062\n",
            "Epoch 5/5\n",
            "343536/343536 [==============================] - 76s 221us/step - loss: 5.0682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS87t37b7ESp",
        "colab_type": "code",
        "outputId": "165a0e6b-f306-47c5-e761-3b398eed76ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "print(\"Generation text : RNN\")\n",
        "for i in range(5):\n",
        "  print(Neural_generator(rnn_word_model, rnn_model))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generation text : RNN\n",
            "<s> deals t break okay if the plaque planet government preparing cell\n",
            "<s> in motorcycle equipment moines 500 <s> take simplify just wheelhouse does\n",
            "<s> steal calling grabs same executive heh lower loss times easing prefer\n",
            "<s> for test jets spent refugees vladimir deductions joke an stagnant letter\n",
            "<s> together bonded brings rude vision leader truth forum potential expression 17th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMwGrNzf3IKW",
        "colab_type": "code",
        "outputId": "958e1e48-579f-4800-d2bf-c0eda45d1e15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "print(\"Generation text : LSTM\")\n",
        "for i in range(5):\n",
        "  print(Neural_generator(lstm_word_model, lstm_model))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generation text : LSTM\n",
            "<s> corporate sell precedent energy cheaply religion financing league discussing cranes missile\n",
            "<s> wait ireland you get player ought calling fans home very affects\n",
            "<s> bad ensure negative landscape within decapitating much isis right bought rude\n",
            "<s> using highest joking mess space president 15 personal community watches scheduled\n",
            "<s> follow tea passion agreement admit theory barn interesting of coast weight\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDDTYfXO8kiR",
        "colab_type": "text"
      },
      "source": [
        "**Observation : Readabilty of LSTM > RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m0bAoeqrC7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_rnn=Neural_perplexity(rnn_model, test)\n",
        "p_lstm=Neural_perplexity(lstm_model, test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKlBsXJNrRGe",
        "colab_type": "code",
        "outputId": "cd6ef85f-44f7-44c4-d537-210e23dcb966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(\"Perplexity RNN : \"+str(p_rnn))\n",
        "print(\"Perplexity LSTM : \"+str(p_lstm))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity RNN : 1727.7609184691341\n",
            "Perplexity LSTM : 1382.8191686225555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8TdAHovwaRZ",
        "colab_type": "text"
      },
      "source": [
        "**Observation : Perplexity of RNN > LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a76U-P-FncRK",
        "colab_type": "text"
      },
      "source": [
        "**Observation : Neural Models perform better than Classical Models. Because of the recurrence property of these RNN Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8t6Pio27G8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}